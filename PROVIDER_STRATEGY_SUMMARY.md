# Provider Abstraction Strategy - Quick Summary

## Current State: Dataset-Centric Type-Driven Design

### Core Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  User Code (Provider-Agnostic)                                 â”‚
â”‚  â”œâ”€â”€ Datasets (with optional storage config)                   â”‚
â”‚  â”œâ”€â”€ Tasks (with compute config)                               â”‚
â”‚  â””â”€â”€ Pipeline (DAG auto-inferred from signatures)              â”‚
â”‚                                                                 â”‚
â”‚  â†“ Configuration carries through...                            â”‚
â”‚                                                                 â”‚
â”‚  Compile/Execution Phase (Provider Selection)                  â”‚
â”‚  â”œâ”€â”€ Local Execution                                           â”‚
â”‚  â”œâ”€â”€ AWS Compilation (Terraform/CloudFormation)               â”‚
â”‚  â”œâ”€â”€ GCP Compilation (Terraform/GCP-specific)                 â”‚
â”‚  â””â”€â”€ Azure Compilation (Terraform/ARM templates)              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Storage Abstraction Strategy

### Design Pattern (Not Yet Implemented)
```
Dataset
â”œâ”€â”€ name: str
â”œâ”€â”€ storage: StorageResource (OPTIONAL)
â”‚   â”œâ”€â”€ S3(bucket, prefix, region)
â”‚   â”œâ”€â”€ GCS(bucket, path, project_id)
â”‚   â”œâ”€â”€ AzureBlob(container, path, account)
â”‚   â””â”€â”€ LocalFS(base_path)
â”œâ”€â”€ schema: Any
â””â”€â”€ metadata: Dict
```

### Key Principle: Late Binding
- **Definition Time**: Declare datasets with optional storage
- **Compile Time**: Select target provider
- **Runtime**: Provider adapter materializes actual storage operations

## Compute Abstraction Strategy

### Fully Implemented Resource Hierarchy
```
ComputeResource (ABC)
â”‚
â”œâ”€â”€ Local(workers=1)
â”‚   â””â”€â”€ Maps to: local process execution
â”‚
â”œâ”€â”€ Container(image, cpu, memory, gpu, env)
â”‚   â””â”€â”€ Maps to: Docker, K8s, ECS, Cloud Run, Container Instances
â”‚
â””â”€â”€ Serverless(memory, timeout, runtime, env)
    â””â”€â”€ Maps to: Lambda, Cloud Functions, Azure Functions
```

### Provider Mapping
| Compute Type | Local | AWS | GCP | Azure |
|---|---|---|---|---|
| Local | âœ“ | âœ— | âœ— | âœ— |
| Container | Docker | ECS | Cloud Run | Container Inst. |
| Serverless | Local Func. | Lambda | Functions | Functions |

## Evolution: Old vs. New Design

### Old Design (Provider Factory Pattern)
```python
# Explicit provider instantiation
provider = AWSProvider(region="us-east-1")
bucket = provider.bucket("data", path="file.parquet")
serverless = provider.serverless("process", handler=fn)

# Cloud knowledge in pipeline code
@task
def process(source: Bucket) -> Bucket:
    return source.scan().filter(...)
```

**Characteristics**:
- Requires provider instantiation before pipeline definition
- DAG implicit in pipeline builder pattern
- Tight coupling to execution phase
- Resource objects tied to provider instance

### New Design (Type-Driven)
```python
# Zero provider imports
raw_data = Dataset("raw_data", 
    storage=S3(bucket="data", prefix="raw/"))
clean_data = Dataset("clean_data")

@task(compute=compute.serverless(memory=1024))
def extract() -> raw_data:
    return fetch_from_api()

@task(compute=compute.local())
def clean(data: raw_data) -> clean_data:
    return process(data)

pipeline = Pipeline([extract, clean])
```

**Characteristics**:
- Zero provider imports in user code
- DAG explicit from type annotations
- Deferred provider selection (compile/execution time)
- Type-safe dataset references

## Configuration Entry Points

### 1. Dataset Configuration
```python
dataset = Dataset(
    name="data",
    storage=S3(bucket="...", prefix="..."),  # Storage
    schema=DataSchema,                         # Schema validation
    metadata={...}                             # Custom metadata
)
```

### 2. Task Configuration
```python
@task(
    compute=compute.serverless(memory=1024, timeout=300),
    # Future: retries, alerts, monitoring
)
def my_task(data: input_data) -> output_data:
    pass
```

### 3. Execution Configuration (Planned)
```python
# At execution time
result = pipeline.run(executor=AWSExecutor(region="us-east-1"))

# Or compile to infrastructure
compiled = pipeline.compile(target=AWSTarget())
compiled.to_terraform("./infra")
```

## Implementation Status

### Completed (âœ…)
- Dataset class with storage/schema/metadata slots
- Task decorator with signature inspection
- Pipeline with automatic DAG inference (DAG from type hints)
- ComputeResource ABC
- Local, Container, Serverless implementations
- Complex DAG support (fan-in, fan-out, diamonds)
- Pipeline validation & visualization

### In Progress (ðŸš§)
- StorageResource interface definition
- Storage implementations (S3, GCS, Azure, Local)
- Execution engines (local, distributed)
- Infrastructure compilation targets

### Future (âš ï¸)
- Terraform generation
- Kubernetes manifest generation
- Schema validation
- Data lineage tracking
- Cost optimization
- Multi-tenant support

## Key Strategic Decisions

### 1. Type-Driven Over API-Driven
```python
# Type hints declare data dependencies automatically
def task(users: users_dataset, orders: orders_dataset) -> merged_dataset:
    # Python type system is the contract language
    pass
```

### 2. Declaration Over Configuration
```python
# Declare what you want, not how to implement it
compute=compute.serverless(memory=1024)  # Need: 1GB memory serverless execution
# Not: LambdaConfig(FunctionName=..., Role=..., ...)
```

### 3. Late Binding of Providers
```python
# Write once, run everywhere
pipeline = Pipeline([tasks...])  # Provider-agnostic

# Different providers selected at execution time
pipeline.run(executor=LocalExecutor())      # Local development
pipeline.compile(target=AWSTarget())        # AWS production
pipeline.compile(target=KubernetesTarget()) # K8s production
```

### 4. Minimal Core
```python
# Core library has ZERO cloud dependencies
from glacier import Dataset, task, Pipeline, compute
# All three compute types work with any cloud provider
```

## Architecture Patterns

### Storage Pattern (Planned)
```
User Code
  â†“
Dataset(storage=S3(...))
  â†“ (compile time)
S3Config â†’ S3BucketResource
  â†“ (execution time)
S3BucketAdapter â†’ boto3 calls
```

### Compute Pattern (Implemented)
```
User Code
  â†“
@task(compute=compute.serverless(...))
  â†“ (compile time, examine get_type())
ServerlessDescriptor â†’ Terraform/K8s template
  â†“ (execution time)
LambdaExecutor/CloudFunctionExecutor â†’ actual invocation
```

## Benefits of Current Design

1. **True Provider Agnosticism** - Same code, any cloud
2. **Type Safety** - IDE autocompletion for datasets
3. **Declarative** - Declare what, not how
4. **Testability** - Can test without cloud provider
5. **Natural Python** - Type hints are already a language feature
6. **Infrastructure from Code** - Config embedded in definitions
7. **Flexible Deployment** - dev (local) â†’ test (cloud) â†’ prod (cloud)

## Next Implementation Steps

1. Define StorageResource ABC
2. Implement S3, GCS, Azure, Local storage adapters
3. Create execution engines (local, distributed)
4. Build compilation targets (Terraform, K8s, etc.)
5. Add schema validation
6. Implement data lineage tracking

---

*For detailed analysis, see PROVIDER_ABSTRACTION_ANALYSIS.md*
